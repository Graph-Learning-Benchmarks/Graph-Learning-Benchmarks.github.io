<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Workshop on Graph Learning Benchmarks (GLB 2021) | GLB 2021</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="@ The Web Conference 2021">
    <meta name="generator" content="Hugo 0.78.1" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    
    
      <link href="/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
      <link href="/index.xml" rel="alternate" type="application/rss+xml" title="GLB 2021" />
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="GLB 2021" />
      
    
    
    <meta property="og:title" content="Workshop on Graph Learning Benchmarks (GLB 2021)" />
<meta property="og:description" content="@ The Web Conference 2021" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://graph-learning-benchmarks.github.io/" />

<meta itemprop="name" content="Workshop on Graph Learning Benchmarks (GLB 2021)">
<meta itemprop="description" content="@ The Web Conference 2021">
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Workshop on Graph Learning Benchmarks (GLB 2021)"/>
<meta name="twitter:description" content="@ The Web Conference 2021"/>

      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-183489314-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
	
  </head>

  <body class="ma0 avenir bg-near-white">

    

  
  
  <header class="cover bg-top" style="background-image: url('https://graph-learning-benchmarks.github.io/images/glb-bg.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        GLB 2021
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/#overview" title="Overview page">
              Overview
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/#call-for-papers" title="Call for Papers page">
              Call for Papers
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/#organizers" title="Organizers page">
              Organizers
            </a>
          </li>
          
        </ul>
      
      















    </div>
  </div>
</nav>

      <div class="tc-l pv4 pv6-l ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">
          Workshop on Graph Learning Benchmarks (GLB 2021)
        </h1>
        
          <h2 class="fw1 f5 f3-l white-80 measure-wide-l center mt3">
            @ The Web Conference 2021
          </h2>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf ph3 ph5-l pv3 pv4-l f4 mw9 lh-copy mid-gray">
    <h1 id="overview">Overview</h1>
<p>Graph-structured data are ubiquitous, heterogeneous, and diverse in real world applications. There are a variety of machine learning tasks formulated on top of the graph-structured data. While recent literature has demonstrated that graph machine learning models&mdash;especially graph neural networks (GNNs)&mdash;achieve promising performance on many such tasks [1,2,3], there is a trend that further improvements on particular tasks often face dramatically different technical bottlenecks. For example, there is a clear distinction between node-level prediction tasks and graph-level prediction tasks: the expressive power of GNNs is mainly a concern for graph-level prediction tasks, and the recently developed higher-order GNNs with better expressive power are almost entirely evaluated on graph-level prediction tasks [4,5,6]; on the other hand, techniques that help GNNs to avoid the over-smoothing problem [7] are mainly tested on node-level prediction tasks [8,9]. Furthermore, among the node-level prediction tasks, a couple of recent works have identified that the standard way of GNNs training implicitly assumes conditional independence among the node labels [10,11], and thus prevents the GNNs from fully leveraging the graph information on some tasks [12,13]. There are also studies revealing that common GNNs only work well when the node labels are smooth or the graphs exhibit homophily [14,15,16].</p>
<p>Unfortunately, despite the clear heterogeneity of graph machine learning tasks and their associated technical challenges, there lack enough quantity and diversity of benchmark tasks and datasets reflecting the rich heterogeneity. The lack of benchmarks not only hinders the community&rsquo;s progress in finding the synergy among the variety of models, it may even bias the development of new models towards narrow directions: there might be an implicit mutual selection procedure between the benchmark datasets and the development of models in the publications. The pitfalls of mainstream models, the advantages of alternatives, and the special needs for new models may all be concealed by the restricted existing benchmarks. When practitioners apply conclusions from the literature to a new task in their own context, they may be surprised by the mismatch between the known results and the reality.</p>
<p>We attribute the aforementioned problems partially to the lack of a comprehensive understanding of the meta-knowledge of graph machine learning tasks. As a comparison, in computer vision, there have been well-categorized tasks in the Computing Classification System (CCS), such as object detection, image segmentation, tracking, etc. And the meta-knowledge of these tasks, e.g., the major technical challenges, are better understood compared to graph learning. A similar trend can be observed in natural language processing, where researchers have a much better understanding of the difficulty of different tasks (e.g., part-of-speech tagging, parsing, entity extraction, machine translation, textual entailment, Web search, etc.), as well as the behaviors of different families of models on each task. In both computer vision and natural language processing, there exist a rich collection of benchmark datasets for almost every task, compared to a rather narrow group of benchmarks for graph learning (e.g., [17,18,19])&mdash;partially because these communities have long recognized the importance of establishing new benchmark datasets and tasks for research progress, and have established dedicated conference tracks that encourage these types of (non-methodological) contributions.</p>
<p>In consideration of these issues, this workshop aims to initiate an effort to obtain diverse graph benchmark tasks and datasets, and gain meta-knowledge of these tasks.  In particular, we propose to call for contributions by establishing novel machine learning tasks on novel graph-structured data which have the potential to provide benchmark evaluations for various graph neural network models. The acceptance of the contributed papers will be decided on the meaningfulness of the established graph learning tasks/datasets and their potential of being formalized into new benchmarks, rather than the performance of machine learning models (old or new) on these tasks. Importantly, contributions of negative results of popular, state-of-the-art models on a new task/dataset are particularly welcome, as these provide novel insights to the community&rsquo;s understanding of the meta-knowledge of graph machine learning.  The benefits of these contributions are three-fold:</p>
<ul>
<li>crowdsourcing benchmark datasets for various tasks of graph machine learning;</li>
<li>identifying systematic failure modes of existing GNNs and providing new technical challenges for the development of new models, thus highlighting diverse future directions;</li>
<li>raising the attention of the synergy of graph learning: e.g., explicitly categorizing tasks on graph data, identifying the unique technical challenges associated with them, and describing the common behavior patterns of existing models.</li>
</ul>
<h1 id="call-for-papers">Call for Papers</h1>
<p>TBD</p>
<h1 id="organizers">Organizers</h1>
<ul>
<li><a href="https://ericdongyx.github.io/">Yuxiao Dong</a>, Facebook AI</li>
<li><a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, University of Michigan</li>
<li><a href="http://www.jiaqima.com/">Jiaqi Ma</a>, University of Michigan</li>
<li><a href="http://www-personal.umich.edu/~qmei/">Qiaozhu Mei</a>, University of Michigan</li>
<li><a href="https://www.jiongzhu.net/">Jiong Zhu</a>, University of Michigan</li>
</ul>
<hr>
<h2 id="references">References</h2>
<ul>
<li>[1] Kipf, T., and Welling, M., 2017.  Semi-supervised classification with graph convolutional networks. ICLR 2017.</li>
<li>[2] Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O. and Dahl, G.E., 2017. Neural message passing for quantum chemistry. ICML 2017.</li>
<li>[3] Yu, B., Yin, H. and Zhu, Z., 2018. Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting. IJCAI 2018.</li>
<li>[4] Morris, C., Ritzert, M., Fey, M., Hamilton, W.L., Lenssen, J.E., Rattan, G. and Grohe, M., 2019. Weisfeiler and leman go neural: Higher-order graph neural networks. AAAI 2019.</li>
<li>[5] Xu, K., Hu, W., Leskovec, J. and Jegelka, S., 2018. How Powerful are Graph Neural Networks?. ICLR 2019.</li>
<li>[6] Maron, H., Ben-Hamu, H., Serviansky, H. and Lipman, Y., 2019. Provably powerful graph networks. NeurIPS 2019.</li>
<li>[7] Li, Q., Han, Z. and Wu, X.M., 2018. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) (pp. 3538-3545). Association for the Advancement of Artificial Intelligence.</li>
<li>[8] Rong, Y., Huang, W., Xu, T. and Huang, J., 2019. Dropedge: Towards deep graph convolutional networks on node classification. ICLR 2019.</li>
<li>[9] Zhao, L. and Akoglu, L., 2019. PairNorm: Tackling Oversmoothing in GNNs. ICLR 2019.</li>
<li>[10] Qu, M., Bengio, Y. and Tang, J., 2019. GMNN: Graph Markov Neural Networks. ICML 2019.</li>
<li>[11] Ma, J., Tang, W., Zhu, J. and Mei, Q., 2019. A Flexible Generative Framework for Graph-based Semi-supervised Learning. NeurIPS 2019.</li>
<li>[12] Jia, J. and Benson, A.R., 2020. Residual Correlation in Graph Neural Network Regression. KDD 2020.</li>
<li>[13] Ma, J., Chang, B., Zhang, X. and Mei, Q., 2020. CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks. arXiv preprint arXiv:2010.02089.</li>
<li>[14] NT, H. and Maehara, T., 2019. Revisiting graph neural networks: All we have is low-pass filters. arXiv preprint arXiv:1905.09550.</li>
<li>[15] Hou, Y., Zhang, J., Cheng, J., Ma, K., Ma, R.T., Chen, H. and Yang, M.C., 2019. Measuring and improving the use of graph information in graph neural networks. ICLR 2019.</li>
<li>[16] Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L. and Koutra, D., 2020. Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. NeurIPS 2020.</li>
<li>[17] Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B. and Eliassi-Rad, T., 2008. Collective classification in network data. AI magazine, 29(3), pp.93-93.</li>
<li>[18] Namata, G., London, B., Getoor, L., Huang, B. and EDU, U., 2012. Query-driven active surveying for collective classification. In 10th International Workshop on Mining and Learning with Graphs (Vol. 8).</li>
<li>[19] Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M. and Leskovec, J., 2020. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687.</li>
</ul>

  </article>
  
  
  
  
  
  
  

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://graph-learning-benchmarks.github.io" >
    &copy;  GLB 2021 
  </a>
    <div>














</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
